{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Initialize FAISS index (dimension 768 for GPT-2 embeddings)\n",
    "index = faiss.IndexFlatL2(768)\n",
    "\n",
    "# Example function to add embeddings to FAISS\n",
    "def add_embeddings_to_faiss(history):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "    for message in history:\n",
    "        inputs = tokenizer(message, return_tensors=\"pt\")\n",
    "        embedding = model(**inputs).last_hidden_state.mean(dim=1).detach().numpy()\n",
    "        index.add(embedding)\n",
    "\n",
    "# Example function to retrieve relevant history\n",
    "def retrieve_relevant_history(query):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Encode query\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "    query_embedding = model(**inputs).last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "    # Search for the nearest neighbors\n",
    "    D, I = index.search(query_embedding, k=3)  # k=3 nearest neighbors\n",
    "    return I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports and API key variables\n",
    "import openai\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import numpy as np\n",
    "\n",
    "# Set up API keys\n",
    "OPENAI_API_KEY = 'your key'\n",
    "openai.api_key = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define the conversation history\n",
    "history = [\n",
    "    \"1: User: Hi there! How are you doing today? | Bot: Hello! I'm doing great, thank you! How can I assist you today?\",\n",
    "    \"2: User: What's the weather like today in New York? | Bot: Today in New York, it's sunny with a slight chance of rain.\",\n",
    "    \"3: User: Great! Do you have any good lunch suggestions? | Bot: Sure! How about trying a new salad recipe?\",\n",
    "    \"4: User: That sounds healthy. Any specific recipes? | Bot: You could try a quinoa salad with avocado and chicken.\",\n",
    "    \"5: User: Sounds delicious! I'll try it. What about dinner? | Bot: For dinner, you could make grilled salmon with vegetables.\",\n",
    "    \"6: User: Thanks for the suggestions! Any dessert ideas? | Bot: How about a simple fruit salad or yogurt with honey?\",\n",
    "    \"7: User: Perfect! Now, what are some good exercises? | Bot: You can try a mix of cardio and strength training exercises.\",\n",
    "    \"8: User: Any specific recommendations for cardio? | Bot: Running, cycling, and swimming are all excellent cardio exercises.\",\n",
    "    \"9: User: I'll start with running. Can you recommend any books? | Bot: 'Atomic Habits' by James Clear is a highly recommended book.\",\n",
    "    \"10: User: I'll check it out. What hobbies can I take up? | Bot: You could explore painting, hiking, or learning a new instrument.\",\n",
    "    # Continue for all 20 messages\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Define the function to add embeddings to FAISS\n",
    "def add_embeddings_to_faiss(history):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "    \n",
    "    # Initialize FAISS index (768 dimensions for GPT-2 embeddings)\n",
    "    index = faiss.IndexFlatL2(768)\n",
    "    \n",
    "    for idx, message in enumerate(history):\n",
    "        inputs = tokenizer(message, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            embedding = model(**inputs).last_hidden_state.mean(dim=1).detach().numpy()\n",
    "        \n",
    "        # Add embedding to FAISS index\n",
    "        index.add(embedding)\n",
    "    \n",
    "    return index\n",
    "\n",
    "# Create FAISS index with history embeddings\n",
    "faiss_index = add_embeddings_to_faiss(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Define the RAG mechanism using FAISS\n",
    "def retrieve_relevant_history(query, faiss_index):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Encode the query\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model(**inputs).last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "    # Query FAISS index for the top 3 most relevant messages\n",
    "    distances, indices = faiss_index.search(query_embedding, k=3)\n",
    "    \n",
    "    # Retrieve the corresponding messages\n",
    "    relevant_messages = [history[i] for i in indices[0]]\n",
    "    \n",
    "    return relevant_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Function to prepare the prompt\n",
    "def prepare_prompt(test_prompt, faiss_index):\n",
    "    # Retrieve relevant history messages using the RAG mechanism\n",
    "    relevant_messages = retrieve_relevant_history(test_prompt, faiss_index)\n",
    "    context = \" \".join(relevant_messages)\n",
    "    \n",
    "    # Combine the retrieved messages with the test prompt\n",
    "    combined_prompt = f\"{context} {test_prompt}\"\n",
    "    \n",
    "    # Ensure the combined prompt does not exceed the 225 token limit\n",
    "    if len(combined_prompt.split()) > 225:\n",
    "        combined_prompt = combined_prompt[:225]\n",
    "    \n",
    "    return combined_prompt, relevant_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Function to test the final prompt\n",
    "def test_final_prompt():\n",
    "    final_test_prompt = \"Do you think it will help me stay fit?\"\n",
    "    \n",
    "    # Prepare the prompt by retrieving relevant history\n",
    "    prepared_prompt, context_referred = prepare_prompt(final_test_prompt, faiss_index)\n",
    "    \n",
    "    # Call OpenAI to generate a response\n",
    "    response = openai.chat.completions.create(\n",
    "         model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": final_test_prompt}\n",
    "            ],\n",
    "            max_tokens=50\n",
    "    )\n",
    "    \n",
    "    # Print the final test prompt, context, and response\n",
    "    print(f\"Final Test Prompt: {final_test_prompt}\")\n",
    "    print(f\"Context Referred: {context_referred}\")\n",
    "    print(f\"Final Test Prompt Response: {response.choices[0].text.strip()}\")\n",
    "\n",
    "# Call the test function\n",
    "test_final_prompt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def test_final_prompt_simple():\n",
    "    final_test_prompt = \"Do you think it will help me stay fit?\"\n",
    "\n",
    "    try:\n",
    "        # Directly create a chat-based completion request\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": final_test_prompt}\n",
    "            ],\n",
    "            max_tokens=50\n",
    "        )\n",
    "        \n",
    "        print(f\"Final Test Prompt: {final_test_prompt}\")\n",
    "        print(f\"Final Test Prompt Response: {response.choices[0].message['content'].strip()}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the simplified test function\n",
    "test_final_prompt_simple()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
