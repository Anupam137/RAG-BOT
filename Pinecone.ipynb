{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries\n",
    "!pip install openai pinecone-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# API keys and environment variables\n",
    "OPENAI_API_KEY = 'open ai key'\n",
    "PINECONE_API_KEY = 'pinecone api'\n",
    "PINECONE_ENVIRONMENT = 'us-east-1'\n",
    "PINECONE_INDEX_NAME = 'chat-history' #you can name anything based on your preference\n",
    "MAX_TOKENS = 225\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pinecone_client = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Check if the index exists\n",
    "existing_indexes = pinecone_client.list_indexes()\n",
    "if PINECONE_INDEX_NAME not in existing_indexes:\n",
    "    print(f\"Creating index '{PINECONE_INDEX_NAME}'...\")\n",
    "    pinecone_client.create_index(\n",
    "        name=PINECONE_INDEX_NAME,\n",
    "        dimension=1536,  # Adjust dimension based on model\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(cloud='aws', region=PINECONE_ENVIRONMENT)\n",
    "    )\n",
    "else:\n",
    "    print(f\"Index '{PINECONE_INDEX_NAME}' already exists, connecting to the index.\")\n",
    "\n",
    "# Connect to the index\n",
    "index = pinecone_client.Index(PINECONE_INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conversation history with each message approximately 20 tokens long\n",
    "history = [\n",
    "    \"1: User: Hi there! How are you doing today? | Bot: Hello! I'm doing great, thank you! How can I assist you today?\",\n",
    "    \"2: User: What's the weather like today in New York? | Bot: Today in New York, it's sunny with a slight chance of rain.\",\n",
    "    \"3: User: Great! Do you have any good lunch suggestions? | Bot: Sure! How about trying a new salad recipe?\",\n",
    "    \"4: User: That sounds healthy. Any specific recipes? | Bot: You could try a quinoa salad with avocado and chicken.\",\n",
    "    \"5: User: Sounds delicious! I'll try it. What about dinner? | Bot: For dinner, you could make grilled salmon with vegetables.\",\n",
    "    \"6: User: Thanks for the suggestions! Any dessert ideas? | Bot: How about a simple fruit salad or yogurt with honey?\",\n",
    "    # Additional conversation data...\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get embeddings using OpenAI API\n",
    "def get_embedding(text):\n",
    "    response = openai.Embedding.create(input=text, model=\"text-embedding-ada-002\")\n",
    "    return response['data'][0]['embedding']\n",
    "\n",
    "# Define the function to add embeddings to Pinecone\n",
    "def add_embeddings_to_pinecone(history, index_name='chat-history'):\n",
    "    \"\"\"\n",
    "    Encodes each message in the history using OpenAI and upserts into Pinecone.\n",
    "    \"\"\"\n",
    "    # Connect to the index\n",
    "    index = pinecone_client.Index(index_name)\n",
    "\n",
    "    # Loop to encode the history and upsert into Pinecone\n",
    "    for idx, message in enumerate(history):\n",
    "        embedding = get_embedding(message)\n",
    "        index.upsert([(str(idx), embedding)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG mechanism to retrieve relevant history\n",
    "def retrieve_relevant_history(query, index_name='chat-history'):\n",
    "    \"\"\"\n",
    "    Encodes the query and retrieves relevant historical messages from Pinecone.\n",
    "    \"\"\"\n",
    "    index = pinecone_client.Index(index_name)\n",
    "    query_embedding = get_embedding(query)\n",
    "    result = index.query(queries=[query_embedding], top_k=5)  # Retrieves top 5 relevant messages\n",
    "    return [match['id'] for match in result['matches']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare the prompt\n",
    "def prepare_prompt(test_prompt, history, index_name='chat-history'):\n",
    "    \"\"\"\n",
    "    Retrieves relevant history using the RAG mechanism, combines it with the test prompt,\n",
    "    and ensures the prompt doesn't exceed the token limit.\n",
    "    \"\"\"\n",
    "    relevant_message_ids = retrieve_relevant_history(test_prompt, index_name)\n",
    "    relevant_history = [history[int(msg_id)] for msg_id in relevant_message_ids]\n",
    "    \n",
    "    combined_prompt = test_prompt + \"\\n\\n\" + \"\\n\".join(relevant_history)\n",
    "    if len(combined_prompt.split()) > MAX_TOKENS:\n",
    "        combined_prompt = \" \".join(combined_prompt.split()[:MAX_TOKENS])\n",
    "    return combined_prompt, relevant_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_final_prompt():\n",
    "    \"\"\"\n",
    "    This function:\n",
    "    1. Defines the final test prompt.\n",
    "    2. Prepares the prompt using the prepare_prompt function.\n",
    "    3. Calls OpenAI to generate a response.\n",
    "    4. Prints the final test prompt, context, and response.\n",
    "    \"\"\"\n",
    "    final_test_prompt = \"Do you think it will help me stay fit?\"\n",
    "    prepared_prompt, context_referred = prepare_prompt(final_test_prompt, history)\n",
    "    \n",
    "    # Call OpenAI API to generate a response\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        prompt=prepared_prompt,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    print(f\"Final Test Prompt: {final_test_prompt}\")\n",
    "    print(f\"Context Referred: {context_referred}\")\n",
    "    print(f\"Final Test Prompt Response: {response.choices[0].text.strip()}\")\n",
    "\n",
    "# Call the test function to generate the Final Test Prompt Response\n",
    "test_final_prompt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
